{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Numalogic \u00b6 Numalogic is a collection of ML models and algorithms for real-time data analytics and AIOps including anomaly detection. Numalogic can be installed as a library and used to build end-to-end ML pipelines. For a streaming real time data processing, it can also be paired with our steaming data platform Numaflow . Key Features \u00b6 Ease of use: simple and efficient tools for predictive data analytics Reusability: all the functionalities can be re-used in various contexts Model selection: easy to compare, validate, fine-tune and choose the model that works best with each data set Data processing: readily available feature extraction, scaling, transforming and normalization tools Extensibility: adding your own functions or extending over the existing capabilities Model Storage: out-of-the-box support for MLFlow and support for other model ML lifecycle management tools Use Cases \u00b6 Deployment failure detection System failure detection for node failures or crashes Fraud detection Network intrusion detection Forecasting on time series data Getting Started \u00b6 For set-up information and running your first pipeline using numalogic, please see our getting started guide .","title":"Home"},{"location":"#numalogic","text":"Numalogic is a collection of ML models and algorithms for real-time data analytics and AIOps including anomaly detection. Numalogic can be installed as a library and used to build end-to-end ML pipelines. For a streaming real time data processing, it can also be paired with our steaming data platform Numaflow .","title":"Numalogic"},{"location":"#key-features","text":"Ease of use: simple and efficient tools for predictive data analytics Reusability: all the functionalities can be re-used in various contexts Model selection: easy to compare, validate, fine-tune and choose the model that works best with each data set Data processing: readily available feature extraction, scaling, transforming and normalization tools Extensibility: adding your own functions or extending over the existing capabilities Model Storage: out-of-the-box support for MLFlow and support for other model ML lifecycle management tools","title":"Key Features"},{"location":"#use-cases","text":"Deployment failure detection System failure detection for node failures or crashes Fraud detection Network intrusion detection Forecasting on time series data","title":"Use Cases"},{"location":"#getting-started","text":"For set-up information and running your first pipeline using numalogic, please see our getting started guide .","title":"Getting Started"},{"location":"autoencoders/","text":"Autoencoders \u00b6 An Autoencoder is a type of Artificial Neural Network, used to learn efficient data representations (encoding) of unlabeled data. It mainly consist of 2 components: an encoder and a decoder. The encoder compresses the input into a lower dimensional code, the decoder then reconstructs the input only using this code. Autoencoder Pipelines \u00b6 Numalogic provides two types of pipelines for Autoencoders. These pipelines serve as a wrapper around the base network models, making it easier to train, predict and generate scores. Also, this module follows the sklearn API. AutoencoderPipeline \u00b6 Here we are using VanillAE , a Vanilla Autoencoder model. from numalogic.models.autoencoder.variants import Conv1dAE from numalogic.models.autoencoder import SparseAEPipeline model = AutoencoderPipeline ( model = VanillaAE ( signal_len = 12 , n_features = 3 ), seq_len = seq_len ) model . fit ( X_train ) SparseAEPipeline \u00b6 A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer. So, by adding a sparsity regularization, we will be able to stop the neural network from copying the input and reduce overfitting. from numalogic.models.autoencoder.variants import Conv1dAE from numalogic.models.autoencoder import SparseAEPipeline model = SparseAEPipeline ( model = VanillaAE ( signal_len = 12 , n_features = 3 ), seq_len = 36 , num_epochs = 30 ) model . fit ( X_train ) Autoencoder Variants \u00b6 Numalogic supports the following variants of Autoencoders VanillaAE \u00b6 Vanilla Autoencoder model comprising only fully connected layers. from numalogic.models.autoencoder.variants import VanillaAE model = VanillaAE ( seq_len = 12 , n_features = 2 ) Conv1dAE \u00b6 Conv1dAE is a one dimensional Convolutional Autoencoder with multichannel support. from numalogic.models.autoencoder.variants import Conv1dAE model = Conv1dAE ( in_channels = 3 , enc_channels = 8 ) LSTMAE \u00b6 An LSTM (Long Short-Term Memory) Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture. from numalogic.models.autoencoder.variants import LSTMAE model = LSTMAE ( seq_len = 12 , no_features = 2 , embedding_dim = 15 ) TransformerAE \u00b6 The transformer-based Autoencoder model was inspired from Attention is all you need paper. It consists of an encoder and a decoder which are both stacks of residual attention blocks, i.e a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. These blocks can process an input sequence of variable length n without exhibiting a recurrent structure and allows transformer-based encoder-decoders to be highly parallelizable. from numalogic.models.autoencoder.variants import TransformerAE model = TransformerAE ( num_heads = 8 , seq_length = 12 , dim_feedforward = 64 , num_encoder_layers = 3 , num_decoder_layers = 1 , )","title":"Autoencoders"},{"location":"autoencoders/#autoencoders","text":"An Autoencoder is a type of Artificial Neural Network, used to learn efficient data representations (encoding) of unlabeled data. It mainly consist of 2 components: an encoder and a decoder. The encoder compresses the input into a lower dimensional code, the decoder then reconstructs the input only using this code.","title":"Autoencoders"},{"location":"autoencoders/#autoencoder-pipelines","text":"Numalogic provides two types of pipelines for Autoencoders. These pipelines serve as a wrapper around the base network models, making it easier to train, predict and generate scores. Also, this module follows the sklearn API.","title":"Autoencoder Pipelines"},{"location":"autoencoders/#autoencoderpipeline","text":"Here we are using VanillAE , a Vanilla Autoencoder model. from numalogic.models.autoencoder.variants import Conv1dAE from numalogic.models.autoencoder import SparseAEPipeline model = AutoencoderPipeline ( model = VanillaAE ( signal_len = 12 , n_features = 3 ), seq_len = seq_len ) model . fit ( X_train )","title":"AutoencoderPipeline"},{"location":"autoencoders/#sparseaepipeline","text":"A Sparse Autoencoder is a type of autoencoder that employs sparsity to achieve an information bottleneck. Specifically the loss function is constructed so that activations are penalized within a layer. So, by adding a sparsity regularization, we will be able to stop the neural network from copying the input and reduce overfitting. from numalogic.models.autoencoder.variants import Conv1dAE from numalogic.models.autoencoder import SparseAEPipeline model = SparseAEPipeline ( model = VanillaAE ( signal_len = 12 , n_features = 3 ), seq_len = 36 , num_epochs = 30 ) model . fit ( X_train )","title":"SparseAEPipeline"},{"location":"autoencoders/#autoencoder-variants","text":"Numalogic supports the following variants of Autoencoders","title":"Autoencoder Variants"},{"location":"autoencoders/#vanillaae","text":"Vanilla Autoencoder model comprising only fully connected layers. from numalogic.models.autoencoder.variants import VanillaAE model = VanillaAE ( seq_len = 12 , n_features = 2 )","title":"VanillaAE"},{"location":"autoencoders/#conv1dae","text":"Conv1dAE is a one dimensional Convolutional Autoencoder with multichannel support. from numalogic.models.autoencoder.variants import Conv1dAE model = Conv1dAE ( in_channels = 3 , enc_channels = 8 )","title":"Conv1dAE"},{"location":"autoencoders/#lstmae","text":"An LSTM (Long Short-Term Memory) Autoencoder is an implementation of an autoencoder for sequence data using an Encoder-Decoder LSTM architecture. from numalogic.models.autoencoder.variants import LSTMAE model = LSTMAE ( seq_len = 12 , no_features = 2 , embedding_dim = 15 )","title":"LSTMAE"},{"location":"autoencoders/#transformerae","text":"The transformer-based Autoencoder model was inspired from Attention is all you need paper. It consists of an encoder and a decoder which are both stacks of residual attention blocks, i.e a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. These blocks can process an input sequence of variable length n without exhibiting a recurrent structure and allows transformer-based encoder-decoders to be highly parallelizable. from numalogic.models.autoencoder.variants import TransformerAE model = TransformerAE ( num_heads = 8 , seq_length = 12 , dim_feedforward = 64 , num_encoder_layers = 3 , num_decoder_layers = 1 , )","title":"TransformerAE"},{"location":"data-generator/","text":"Data Generator \u00b6 Numalogic provides a data generator to create some synthetic time series data, that can be used as train or test data sets. Using the synthetic data, we can: Compare and evaluate different ML algorithms, since we have labeled anomalies Understand different types of anomalies, and our models' performance on each of them Recreate realtime scenarios Generate multivariate timeseries \u00b6 from numalogic.synthetic import SyntheticTSGenerator ts_generator = SyntheticTSGenerator ( seq_len = 8000 , num_series = 3 , freq = \"T\" , primary_period = 720 , secondary_period = 6000 , seasonal_ts_prob = 0.8 , baseline_range = ( 200.0 , 350.0 ), slope_range = ( - 0.001 , 0.01 ), amplitude_range = ( 10 , 75 ), cosine_ratio_range = ( 0.5 , 0.9 ), noise_range = ( 5 , 15 ), ) # shape: (8000, 3) with column names [s1, s2, s3] ts_df = ts_generator . gen_tseries () # Split into test and train train_df , test_df = ts_generator . train_test_split ( ts_df , test_size = 1000 ) Inject anomalies \u00b6 Now, once we generate the synthetic data like above, we can inject anomalies into the test data set using AnomalyGenerator . AnomalyGenerator supports the following types of anomalies: global: Outliers in the global context contextual: Outliers only in the seasonal context causal: Outliers caused by a temporal causal effect collective: Outliers present simultaneously in two or more time series You can also use anomaly_ratio to adjust the ratio of anomalous data points wrt number of samples. from numalogic.synthetic import AnomalyGenerator # columns to inject anomalies injected_cols = [ \"s1\" , \"s2\" ] anomaly_generator = AnomalyGenerator ( train_df , anomaly_type = \"contextual\" , anomaly_ratio = 0.3 ) outlier_test_df = anomaly_generator . inject_anomalies ( test_df , cols = injected_cols , impact = 1.5 )","title":"Data Generator"},{"location":"data-generator/#data-generator","text":"Numalogic provides a data generator to create some synthetic time series data, that can be used as train or test data sets. Using the synthetic data, we can: Compare and evaluate different ML algorithms, since we have labeled anomalies Understand different types of anomalies, and our models' performance on each of them Recreate realtime scenarios","title":"Data Generator"},{"location":"data-generator/#generate-multivariate-timeseries","text":"from numalogic.synthetic import SyntheticTSGenerator ts_generator = SyntheticTSGenerator ( seq_len = 8000 , num_series = 3 , freq = \"T\" , primary_period = 720 , secondary_period = 6000 , seasonal_ts_prob = 0.8 , baseline_range = ( 200.0 , 350.0 ), slope_range = ( - 0.001 , 0.01 ), amplitude_range = ( 10 , 75 ), cosine_ratio_range = ( 0.5 , 0.9 ), noise_range = ( 5 , 15 ), ) # shape: (8000, 3) with column names [s1, s2, s3] ts_df = ts_generator . gen_tseries () # Split into test and train train_df , test_df = ts_generator . train_test_split ( ts_df , test_size = 1000 )","title":"Generate multivariate timeseries"},{"location":"data-generator/#inject-anomalies","text":"Now, once we generate the synthetic data like above, we can inject anomalies into the test data set using AnomalyGenerator . AnomalyGenerator supports the following types of anomalies: global: Outliers in the global context contextual: Outliers only in the seasonal context causal: Outliers caused by a temporal causal effect collective: Outliers present simultaneously in two or more time series You can also use anomaly_ratio to adjust the ratio of anomalous data points wrt number of samples. from numalogic.synthetic import AnomalyGenerator # columns to inject anomalies injected_cols = [ \"s1\" , \"s2\" ] anomaly_generator = AnomalyGenerator ( train_df , anomaly_type = \"contextual\" , anomaly_ratio = 0.3 ) outlier_test_df = anomaly_generator . inject_anomalies ( test_df , cols = injected_cols , impact = 1.5 )","title":"Inject anomalies"},{"location":"forecasting/","text":"Forcasting \u00b6 Numalogic supports the following variants of forecasting based anomaly detection models. Naive Forecasters \u00b6 Baseline Forecaster \u00b6 This is a naive forecaster, that uses a combination of: Log transformation Z-Score normalization from numalogic.models.forecast.variants import BaselineForecaster model = BaselineForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df ) Seasonal Naive Forecaster \u00b6 A naive forecaster that takes seasonality into consideration and predicts the previous day/week values. from numalogic.models.forecast.variants import SeasonalNaiveForecaster model = SeasonalNaiveForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df )","title":"Forcasting"},{"location":"forecasting/#forcasting","text":"Numalogic supports the following variants of forecasting based anomaly detection models.","title":"Forcasting"},{"location":"forecasting/#naive-forecasters","text":"","title":"Naive Forecasters"},{"location":"forecasting/#baseline-forecaster","text":"This is a naive forecaster, that uses a combination of: Log transformation Z-Score normalization from numalogic.models.forecast.variants import BaselineForecaster model = BaselineForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df )","title":"Baseline Forecaster"},{"location":"forecasting/#seasonal-naive-forecaster","text":"A naive forecaster that takes seasonality into consideration and predicts the previous day/week values. from numalogic.models.forecast.variants import SeasonalNaiveForecaster model = SeasonalNaiveForecaster () model . fit ( train_df ) pred_df = model . predict ( test_df ) r2_score = model . r2_score ( test_df ) anomaly_score = model . score ( test_df )","title":"Seasonal Naive Forecaster"},{"location":"inference/","text":"Inference \u00b6 Now, once we have the model trained using one of the ML algorthims, we can predict the anomalies in the test data. This can be a streaming or a batched data. X_test = scaler . transform ( outlier_test_df . to_numpy ()) # predict method returns the reconstruction produced by the AE test_recon = model . predict ( X_test ) # score method returns the anomaly score, calculated using thresholds. # A number less than 1 indicates an inlier, and greater than 1 indicates an outlier. test_anomaly_score = model . score ( X_test )","title":"Inference"},{"location":"inference/#inference","text":"Now, once we have the model trained using one of the ML algorthims, we can predict the anomalies in the test data. This can be a streaming or a batched data. X_test = scaler . transform ( outlier_test_df . to_numpy ()) # predict method returns the reconstruction produced by the AE test_recon = model . predict ( X_test ) # score method returns the anomaly score, calculated using thresholds. # A number less than 1 indicates an inlier, and greater than 1 indicates an outlier. test_anomaly_score = model . score ( X_test )","title":"Inference"},{"location":"ml-flow/","text":"MLflow \u00b6 Numalogic has built in support for Mlflow's tracking and logging system. Starting MLflow \u00b6 To start the mlflow server on localhost , which has already been installed optionally via poetry dependency, run the following command. Replace the {directory} with the path you want to save the models. mlflow server \\ --default-artifact-root { directory } /mlruns --serve-artifacts \\ --backend-store-uri sqlite:///mlflow.db --host 0 .0.0.0 --port 5000 Once the mlflow server has been started, you can navigate to http://127.0.0.1:5000/ to explore mlflow UI. Model saving \u00b6 Numalogic provides MLflowRegistrar , to save and load models to/from MLflow. Here, tracking_uri is the uri where mlflow server is running. The static_keys and dynamic_keys are used to form a unique key for the model. The primary_artifact would be the main model, and secondary_artifacts can be used to save any pre-processing models like scalers. from numalogic.registry import MLflowRegistrar # static and dynamic keys are used to look up a model static_keys = [ \"synthetic\" , \"3ts\" ] dynamic_keys = [ \"minmaxscaler\" , \"sparseconv1d\" ] registry = MLflowRegistrar ( tracking_uri = \"http://0.0.0.0:5000\" , artifact_type = \"pytorch\" ) registry . save ( skeys = static_keys , dkeys = dynamic_keys , primary_artifact = model , secondary_artifacts = { \"preproc\" : scaler } ) Model loading \u00b6 Once, the models are save to MLflow, the load function of MLflowRegistrar can be used to load the model. registry = MLflowRegistrar ( tracking_uri = \"http://0.0.0.0:8080\" ) artifact_dict = registry . load ( skeys = static_keys , dkeys = dynamic_keys ) scaler = artifact_dict [ \"secondary_artifacts\" ][ \"preproc\" ] model = artifact_dict [ \"primary_artifact\" ] For more details, please refer to MLflow Model Registry","title":"MLflow"},{"location":"ml-flow/#mlflow","text":"Numalogic has built in support for Mlflow's tracking and logging system.","title":"MLflow"},{"location":"ml-flow/#starting-mlflow","text":"To start the mlflow server on localhost , which has already been installed optionally via poetry dependency, run the following command. Replace the {directory} with the path you want to save the models. mlflow server \\ --default-artifact-root { directory } /mlruns --serve-artifacts \\ --backend-store-uri sqlite:///mlflow.db --host 0 .0.0.0 --port 5000 Once the mlflow server has been started, you can navigate to http://127.0.0.1:5000/ to explore mlflow UI.","title":"Starting MLflow"},{"location":"ml-flow/#model-saving","text":"Numalogic provides MLflowRegistrar , to save and load models to/from MLflow. Here, tracking_uri is the uri where mlflow server is running. The static_keys and dynamic_keys are used to form a unique key for the model. The primary_artifact would be the main model, and secondary_artifacts can be used to save any pre-processing models like scalers. from numalogic.registry import MLflowRegistrar # static and dynamic keys are used to look up a model static_keys = [ \"synthetic\" , \"3ts\" ] dynamic_keys = [ \"minmaxscaler\" , \"sparseconv1d\" ] registry = MLflowRegistrar ( tracking_uri = \"http://0.0.0.0:5000\" , artifact_type = \"pytorch\" ) registry . save ( skeys = static_keys , dkeys = dynamic_keys , primary_artifact = model , secondary_artifacts = { \"preproc\" : scaler } )","title":"Model saving"},{"location":"ml-flow/#model-loading","text":"Once, the models are save to MLflow, the load function of MLflowRegistrar can be used to load the model. registry = MLflowRegistrar ( tracking_uri = \"http://0.0.0.0:8080\" ) artifact_dict = registry . load ( skeys = static_keys , dkeys = dynamic_keys ) scaler = artifact_dict [ \"secondary_artifacts\" ][ \"preproc\" ] model = artifact_dict [ \"primary_artifact\" ] For more details, please refer to MLflow Model Registry","title":"Model loading"},{"location":"post-processing/","text":"Post Processing \u00b6 Post-processing step is again an optional step, where we normalize the anomalies between 0-10. This is mostly to make the scores more understandable. from numalogic.scores import tanh_norm test_anomaly_score_norm = tanh_norm ( test_anomaly_score )","title":"Post Processing"},{"location":"post-processing/#post-processing","text":"Post-processing step is again an optional step, where we normalize the anomalies between 0-10. This is mostly to make the scores more understandable. from numalogic.scores import tanh_norm test_anomaly_score_norm = tanh_norm ( test_anomaly_score )","title":"Post Processing"},{"location":"pre-processing/","text":"Pre Processing \u00b6 When creating a Machine Learning pipeline, data pre-processing plays a crucial role that takes in raw data and transforms it into a format that can be understood and analyzed by the ML Models. Generally, the majority of real-word datasets are incomplete, inconsistent or inaccurate (contains errors or outliers). Applying ML algorithms on this raw data would give inaccurate results, as they would fail to identify the underlying patterns effectively. Quality decisions must be based on quality data. Data Preprocessing is important to get this quality data, without which it would just be a Garbage In, Garbage Out scenario. Numalogic provides the following tranformers for pre-processing the training or testing data sets. You can also pair it with scalers like MinMaxScaler from scikit-learn pre-processing tools. Log Transformer \u00b6 Log transformation is a data transformation method in which it replaces each data point x with a log(x). Now, with add_factor , each data point x is converted to log(x + add_factor) Log transformation reduces the variance in some distributions, especially with large outliers. from sklearn.pipeline import make_pipeline from sklearn.preprocessing import MinMaxScaler from numalogic.preprocess.transformer import LogTransformer transformer = LogTransformer ( add_factor = 1 ) scaler = MinMaxScaler () pipeline = make_pipeline ( transformer , scaler ) X_train = transformer . transform ( train_df . to_numpy ()) X_test = scaler . transform ( test_df . to_numpy ()) Static Power Transformer \u00b6 Static Power Transformer converts each data point x to x n . When add_factor is provided, each data point x is converted to (x + add_factor) n from sklearn.pipeline import make_pipeline from sklearn.preprocessing import MinMaxScaler from numalogic.preprocess.transformer import StaticPowerTransformer transformer = StaticPowerTransformer ( n = 3 , add_factor = 2 ) scaler = MinMaxScaler () pipeline = make_pipeline ( transformer , scaler ) X_train = transformer . transform ( train_df . to_numpy ()) X_test = scaler . transform ( test_df . to_numpy ())","title":"Pre Processing"},{"location":"pre-processing/#pre-processing","text":"When creating a Machine Learning pipeline, data pre-processing plays a crucial role that takes in raw data and transforms it into a format that can be understood and analyzed by the ML Models. Generally, the majority of real-word datasets are incomplete, inconsistent or inaccurate (contains errors or outliers). Applying ML algorithms on this raw data would give inaccurate results, as they would fail to identify the underlying patterns effectively. Quality decisions must be based on quality data. Data Preprocessing is important to get this quality data, without which it would just be a Garbage In, Garbage Out scenario. Numalogic provides the following tranformers for pre-processing the training or testing data sets. You can also pair it with scalers like MinMaxScaler from scikit-learn pre-processing tools.","title":"Pre Processing"},{"location":"pre-processing/#log-transformer","text":"Log transformation is a data transformation method in which it replaces each data point x with a log(x). Now, with add_factor , each data point x is converted to log(x + add_factor) Log transformation reduces the variance in some distributions, especially with large outliers. from sklearn.pipeline import make_pipeline from sklearn.preprocessing import MinMaxScaler from numalogic.preprocess.transformer import LogTransformer transformer = LogTransformer ( add_factor = 1 ) scaler = MinMaxScaler () pipeline = make_pipeline ( transformer , scaler ) X_train = transformer . transform ( train_df . to_numpy ()) X_test = scaler . transform ( test_df . to_numpy ())","title":"Log Transformer"},{"location":"pre-processing/#static-power-transformer","text":"Static Power Transformer converts each data point x to x n . When add_factor is provided, each data point x is converted to (x + add_factor) n from sklearn.pipeline import make_pipeline from sklearn.preprocessing import MinMaxScaler from numalogic.preprocess.transformer import StaticPowerTransformer transformer = StaticPowerTransformer ( n = 3 , add_factor = 2 ) scaler = MinMaxScaler () pipeline = make_pipeline ( transformer , scaler ) X_train = transformer . transform ( train_df . to_numpy ()) X_test = scaler . transform ( test_df . to_numpy ())","title":"Static Power Transformer"},{"location":"quick-start/","text":"Quick Start \u00b6 Installation \u00b6 Install Numalogic and experiment with the different tools available. pip install numalogic Numalogic as a Library \u00b6 Numalogic can be used as an independent library, and it provides various ML models and tools. Here, we are using a AutoencoderPipeline . Refer to training section for other available options. In this example, the train data set has numbers ranging from 1-10. Whereas in the test data set, there are data points that go out of this range, which the algorithm should be able to detect as anomalies. import numpy as np from numalogic.models.autoencoder import AutoencoderPipeline from numalogic.models.autoencoder.variants import Conv1dAE from numalogic.scores import tanh_norm X_train = np . array ([ 1 , 3 , 5 , 2 , 5 , 1 , 4 , 5 , 1 , 4 , 5 , 8 , 9 , 1 , 2 , 4 , 5 , 1 , 3 ]) . reshape ( - 1 , 1 ) X_test = np . array ([ - 20 , 3 , 5 , 40 , 5 , 10 , 4 , 5 , 100 ]) . reshape ( - 1 , 1 ) model = AutoencoderPipeline ( model = Conv1dAE ( in_channels = 1 , enc_channels = 4 ), seq_len = 8 , num_epochs = 30 ) # fit method trains the model on train data set model . fit ( X_train ) # predict method returns the reconstruction error recon = model . predict ( X_test ) # score method returns the anomaly score computed on test data set anomaly_score = model . score ( X_test ) # normalizing scores to range between 0-10 anomaly_score_norm = tanh_norm ( anomaly_score ) print ( \"Anomaly Scores:\" , anomaly_score_norm ) Below is the sample output, which has logs and anomaly scores printed. Notice the anomaly score for points -20, 40 and 100 in X_test is high. ...snip training logs... Anomaly Scores: [[ 2 .70173135 ] [ 0 .22298803 ] [ 0 .01045979 ] [ 3 .66973793 ] [ 0 .12931582 ] [ 0 .53661316 ] [ 0 .10056313 ] [ 0 .2634344 ] [ 7 .76317209 ]] Replace X_train and X_test with your own data, and see the anomaly scores generated. For more detailed experimentation, refer to quick-start example Numalogic as streaming ML using Numaflow \u00b6 Numalogic can also be paired with our streaming platform Numaflow , to build streaming ML pipelines where Numalogic can be used in UDF . Prerequisite \u00b6 Numaflow Running the Simple Numalogic Pipeline \u00b6 Once Numaflow is installed, create a simple Numalogic pipeline, which takes in time-series data, does the pre-processing, training, inference, and post-processing. For building this pipeline, navigate to numalogic-simple-pipeline under the examples folder and execute the following commands. Apply the pipeline. Note Make sure the pipeline and, numaflow controllers and isbsvc pods are running in the same namespace ( default in this case). kubectl apply -f numa-pl.yaml To verify if the pipeline has been deployed successfully, check the status of each pod. kubectl get pods Output will be something like this: NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3/3 Running 0 68s isbsvc-default-js-1 3/3 Running 0 68s isbsvc-default-js-2 3/3 Running 0 68s mlflow-sqlite-84cf5d6cd-pkmct 1/1 Running 0 46s numalogic-simple-pipeline-preprocess-0-mvuqb 2/2 Running 0 46s numalogic-simple-pipeline-train-0-8xjg1 2/2 Running 0 46s numalogic-simple-pipeline-daemon-66bbd94c4-hf4k2 1/1 Running 0 46s numalogic-simple-pipeline-inference-0-n3asg 2/2 Running 0 46s numalogic-simple-pipeline-postprocess-0-bw67q 2/2 Running 0 46s numalogic-simple-pipeline-out-0-hjb7m 1/1 Running 0 46s numalogic-simple-pipeline-in-0-tmd0v 1/1 Running 0 46s Sending data to the pipeline \u00b6 Once the pipeline has been created, the data can be sent to the pipeline by port-forwarding the input vertex. Port-forward to the http-source vertex. From the above pod output, this would be: kubectl port-forward numalogic-simple-pipeline-in-0-tmd0v 8443 Send the data to the pod via curl curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' Note: only send an array of length 12 in data, as the sequence length used for training is 12. Training \u00b6 Initially, there is no ML model present; to trigger training do a curl command and send any data to the pipeline. The training data is from train_data.csv , which follows a sinusoidal pattern where values fall in the range 200-350. The following logs will be seen in the training pod. > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' > kubectl logs numalogic-simple-pipeline-train-0-xxxxx -c udf 2022 -10-19 22 :38:45,431 - INFO - Training autoencoder model.. 2022 -10-19 22 :38:45,783 - INFO - epoch : 5 , loss_mean : 0 .0744678 2022 -10-19 22 :38:46,431 - INFO - epoch : 10 , loss_mean : 0 .0491540 ... 2022 -10-19 22 :38:49,645 - INFO - epoch : 95 , loss_mean : 0 .0047888 2022 -10-19 22 :38:49,878 - INFO - epoch : 100 , loss_mean : 0 .0043651 2022 -10-19 22 :38:49,880 - INFO - 8b597791-b8a3-41b0-8375-47e168887c54 - Training complete Successfully registered model 'ae::model' . 2022 /10/19 22 :38:52 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: ae::model, version 1 Created version '1' of model 'ae::model' . 2022 -10-19 22 :38:52,920 - INFO - 8b597791-b8a3-41b0-8375-47e168887c54 - Model Saving complete Inference \u00b6 Now, the pipeline is ready for inference with the model trained above, data can be sent to the pipeline for ML inference. After sending the data, look for logs in the output pod, which shows the anomaly score. Since we trained the model with data that follows a sinusoidal pattern where values range from 200-350, any value within this range is considered to be non-anomalous. And any value out of this range is considered to be anomalous. Sending non-anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,359.080987,341.036110,333.584121,376.034150,351.065394,355.379422,333.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:54:44 (out) {\"ts_data\": [[0.14472376660734326], [0.638373062689151], [0.8480656378656608], [0.4205087588581154], [1.285475729481929], [0.8136729095134241], [0.09972157219780131], [0.2856860200353754], [0.6005371351085002], [0.021966491476278518], [0.10405302543443251], [0.6428168263777302]], \"anomaly_score\": 0.49173648784304, \"uuid\": \"0506b380-4565-405c-a3a3-ddc3a19e0bb4\"} Sending anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,800.162220,614.091646,537.250124,776.034150,751.065394,700.379422,733.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:56:40 (out) {\"ts_data\": [[1.173712319431301], [0.39061549013480673], [2.523849648503271], [2.0962927694957254], [13.032012667825805], [5.80166091013039], [3.6868855191928325], [4.814846700913904], [4.185973265627947], [3.9097889275446356], [4.505391607282856], [4.1170053183846305]], \"anomaly_score\": 3.9579276751803145, \"uuid\": \"ed039779-f924-4801-9418-eeef30715ef1\"} In the output, ts_data is the final array that the input array has been transformed to, after all the steps in the pipeline. anomaly_score is the final anomaly score generated for the input data. MLflow UI \u00b6 To see the model in MLflow UI, port forward mlflow-service using the below command and navigate to http://127.0.0.1:5000/ kubectl port-forward svc/mlflow-service 5000 Train on your own data \u00b6 If you want to train an ML model on your own data, replace the train_data.csv file with your own file under resources. For more details, refer to numalogic-simple-pipeline","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"","title":"Quick Start"},{"location":"quick-start/#installation","text":"Install Numalogic and experiment with the different tools available. pip install numalogic","title":"Installation"},{"location":"quick-start/#numalogic-as-a-library","text":"Numalogic can be used as an independent library, and it provides various ML models and tools. Here, we are using a AutoencoderPipeline . Refer to training section for other available options. In this example, the train data set has numbers ranging from 1-10. Whereas in the test data set, there are data points that go out of this range, which the algorithm should be able to detect as anomalies. import numpy as np from numalogic.models.autoencoder import AutoencoderPipeline from numalogic.models.autoencoder.variants import Conv1dAE from numalogic.scores import tanh_norm X_train = np . array ([ 1 , 3 , 5 , 2 , 5 , 1 , 4 , 5 , 1 , 4 , 5 , 8 , 9 , 1 , 2 , 4 , 5 , 1 , 3 ]) . reshape ( - 1 , 1 ) X_test = np . array ([ - 20 , 3 , 5 , 40 , 5 , 10 , 4 , 5 , 100 ]) . reshape ( - 1 , 1 ) model = AutoencoderPipeline ( model = Conv1dAE ( in_channels = 1 , enc_channels = 4 ), seq_len = 8 , num_epochs = 30 ) # fit method trains the model on train data set model . fit ( X_train ) # predict method returns the reconstruction error recon = model . predict ( X_test ) # score method returns the anomaly score computed on test data set anomaly_score = model . score ( X_test ) # normalizing scores to range between 0-10 anomaly_score_norm = tanh_norm ( anomaly_score ) print ( \"Anomaly Scores:\" , anomaly_score_norm ) Below is the sample output, which has logs and anomaly scores printed. Notice the anomaly score for points -20, 40 and 100 in X_test is high. ...snip training logs... Anomaly Scores: [[ 2 .70173135 ] [ 0 .22298803 ] [ 0 .01045979 ] [ 3 .66973793 ] [ 0 .12931582 ] [ 0 .53661316 ] [ 0 .10056313 ] [ 0 .2634344 ] [ 7 .76317209 ]] Replace X_train and X_test with your own data, and see the anomaly scores generated. For more detailed experimentation, refer to quick-start example","title":"Numalogic as a Library"},{"location":"quick-start/#numalogic-as-streaming-ml-using-numaflow","text":"Numalogic can also be paired with our streaming platform Numaflow , to build streaming ML pipelines where Numalogic can be used in UDF .","title":"Numalogic as streaming ML using Numaflow"},{"location":"quick-start/#prerequisite","text":"Numaflow","title":"Prerequisite"},{"location":"quick-start/#running-the-simple-numalogic-pipeline","text":"Once Numaflow is installed, create a simple Numalogic pipeline, which takes in time-series data, does the pre-processing, training, inference, and post-processing. For building this pipeline, navigate to numalogic-simple-pipeline under the examples folder and execute the following commands. Apply the pipeline. Note Make sure the pipeline and, numaflow controllers and isbsvc pods are running in the same namespace ( default in this case). kubectl apply -f numa-pl.yaml To verify if the pipeline has been deployed successfully, check the status of each pod. kubectl get pods Output will be something like this: NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3/3 Running 0 68s isbsvc-default-js-1 3/3 Running 0 68s isbsvc-default-js-2 3/3 Running 0 68s mlflow-sqlite-84cf5d6cd-pkmct 1/1 Running 0 46s numalogic-simple-pipeline-preprocess-0-mvuqb 2/2 Running 0 46s numalogic-simple-pipeline-train-0-8xjg1 2/2 Running 0 46s numalogic-simple-pipeline-daemon-66bbd94c4-hf4k2 1/1 Running 0 46s numalogic-simple-pipeline-inference-0-n3asg 2/2 Running 0 46s numalogic-simple-pipeline-postprocess-0-bw67q 2/2 Running 0 46s numalogic-simple-pipeline-out-0-hjb7m 1/1 Running 0 46s numalogic-simple-pipeline-in-0-tmd0v 1/1 Running 0 46s","title":"Running the Simple Numalogic Pipeline"},{"location":"quick-start/#sending-data-to-the-pipeline","text":"Once the pipeline has been created, the data can be sent to the pipeline by port-forwarding the input vertex. Port-forward to the http-source vertex. From the above pod output, this would be: kubectl port-forward numalogic-simple-pipeline-in-0-tmd0v 8443 Send the data to the pod via curl curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' Note: only send an array of length 12 in data, as the sequence length used for training is 12.","title":"Sending data to the pipeline"},{"location":"quick-start/#training","text":"Initially, there is no ML model present; to trigger training do a curl command and send any data to the pipeline. The training data is from train_data.csv , which follows a sinusoidal pattern where values fall in the range 200-350. The following logs will be seen in the training pod. > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[0.9,0.1,0.2,0.9,0.9,0.9,0.9,0.8,1,0.9,0.9,0.7]}' > kubectl logs numalogic-simple-pipeline-train-0-xxxxx -c udf 2022 -10-19 22 :38:45,431 - INFO - Training autoencoder model.. 2022 -10-19 22 :38:45,783 - INFO - epoch : 5 , loss_mean : 0 .0744678 2022 -10-19 22 :38:46,431 - INFO - epoch : 10 , loss_mean : 0 .0491540 ... 2022 -10-19 22 :38:49,645 - INFO - epoch : 95 , loss_mean : 0 .0047888 2022 -10-19 22 :38:49,878 - INFO - epoch : 100 , loss_mean : 0 .0043651 2022 -10-19 22 :38:49,880 - INFO - 8b597791-b8a3-41b0-8375-47e168887c54 - Training complete Successfully registered model 'ae::model' . 2022 /10/19 22 :38:52 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: ae::model, version 1 Created version '1' of model 'ae::model' . 2022 -10-19 22 :38:52,920 - INFO - 8b597791-b8a3-41b0-8375-47e168887c54 - Model Saving complete","title":"Training"},{"location":"quick-start/#inference","text":"Now, the pipeline is ready for inference with the model trained above, data can be sent to the pipeline for ML inference. After sending the data, look for logs in the output pod, which shows the anomaly score. Since we trained the model with data that follows a sinusoidal pattern where values range from 200-350, any value within this range is considered to be non-anomalous. And any value out of this range is considered to be anomalous. Sending non-anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,359.080987,341.036110,333.584121,376.034150,351.065394,355.379422,333.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:54:44 (out) {\"ts_data\": [[0.14472376660734326], [0.638373062689151], [0.8480656378656608], [0.4205087588581154], [1.285475729481929], [0.8136729095134241], [0.09972157219780131], [0.2856860200353754], [0.6005371351085002], [0.021966491476278518], [0.10405302543443251], [0.6428168263777302]], \"anomaly_score\": 0.49173648784304, \"uuid\": \"0506b380-4565-405c-a3a3-ddc3a19e0bb4\"} Sending anomalous data: > curl -kq -X POST https://localhost:8443/vertices/in -d '{\"data\":[358.060687,326.253469,329.023996,346.168602,339.511273,800.162220,614.091646,537.250124,776.034150,751.065394,700.379422,733.347769]}' > kubectl logs numalogic-simple-pipeline-out-0-xxxxx 2022/10/20 04:56:40 (out) {\"ts_data\": [[1.173712319431301], [0.39061549013480673], [2.523849648503271], [2.0962927694957254], [13.032012667825805], [5.80166091013039], [3.6868855191928325], [4.814846700913904], [4.185973265627947], [3.9097889275446356], [4.505391607282856], [4.1170053183846305]], \"anomaly_score\": 3.9579276751803145, \"uuid\": \"ed039779-f924-4801-9418-eeef30715ef1\"} In the output, ts_data is the final array that the input array has been transformed to, after all the steps in the pipeline. anomaly_score is the final anomaly score generated for the input data.","title":"Inference"},{"location":"quick-start/#mlflow-ui","text":"To see the model in MLflow UI, port forward mlflow-service using the below command and navigate to http://127.0.0.1:5000/ kubectl port-forward svc/mlflow-service 5000","title":"MLflow UI"},{"location":"quick-start/#train-on-your-own-data","text":"If you want to train an ML model on your own data, replace the train_data.csv file with your own file under resources. For more details, refer to numalogic-simple-pipeline","title":"Train on your own data"}]}